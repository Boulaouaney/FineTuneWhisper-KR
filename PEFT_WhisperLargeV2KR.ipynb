{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 1.13.1 which is incompatible.\n",
      "torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 1.13.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to C:\\Users\\Youssef\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Bingsu--zeroth-korean-787ca68963c66467\n",
      "Found cached dataset parquet (D:/Youssef/.cache/huggingface/datasets/Bingsu___parquet/Bingsu--zeroth-korean-787ca68963c66467/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration Bingsu--zeroth-korean-787ca68963c66467\n",
      "Found cached dataset parquet (D:/Youssef/.cache/huggingface/datasets/Bingsu___parquet/Bingsu--zeroth-korean-787ca68963c66467/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 22263\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 457\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "Zeroth_Korean = DatasetDict()\n",
    "\n",
    "Zeroth_Korean[\"train\"] = load_dataset(\"Bingsu/zeroth-korean\", split=\"train\", use_auth_token=True, cache_dir=\"D:/Youssef/.cache/huggingface/datasets\")\n",
    "Zeroth_Korean[\"test\"] = load_dataset(\"Bingsu/zeroth-korean\", split=\"test\", use_auth_token=True, cache_dir=\"D:/Youssef/.cache/huggingface/datasets\")\n",
    "\n",
    "print(Zeroth_Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"Korean\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([-3.05175781e-05,  0.00000000e+00, -3.05175781e-05, ...,\n",
      "        0.00000000e+00,  0.00000000e+00, -6.10351562e-05]), 'sampling_rate': 16000}, 'text': '인사를 결정하는 과정에서 당 지도부가 우 원내대표 및 원내지도부와 충분한 상의를 거치지 않은 채 일방적으로 인사를 했다는 불만도 원내지도부를 중심으로 흘러나왔다'}\n"
     ]
    }
   ],
   "source": [
    "print(Zeroth_Korean[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prep(batch):\n",
    "    # load audio\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # calculate log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\Youssef\\.cache\\huggingface\\datasets\\Bingsu___parquet\\Bingsu--zeroth-korean-787ca68963c66467\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-e0812d3231b56f3b.arrow\n",
      "Loading cached processed dataset at D:\\Youssef\\.cache\\huggingface\\datasets\\Bingsu___parquet\\Bingsu--zeroth-korean-787ca68963c66467\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-0f77e663e93fdffc.arrow\n"
     ]
    }
   ],
   "source": [
    "# applying data preparation func to all training examples using .map\n",
    "Zeroth_Korean = Zeroth_Korean.map(dataset_prep, remove_columns=Zeroth_Korean.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 22263\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 457\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(Zeroth_Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "         # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\", cache_dir=\"D:/Youssef/.cache/huggingface/evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\", load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_training\n",
    "\n",
    "model = prepare_model_for_training(model, output_imbedding_layer=\"proj_out\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-Large-v2-KR\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Youssef\\Lab\\FineTuneWhisper-KR\\./whisper-small-KR is already a clone of https://huggingface.co/byoussef/whisper-small-KR. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=Zeroth_Korean[\"train\"],\n",
    "    eval_dataset=Zeroth_Korean[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Should re-enable for inference!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in ./whisper-small-KR\\preprocessor_config.json\n",
      "tokenizer config file saved in ./whisper-small-KR\\tokenizer_config.json\n",
      "Special tokens file saved in ./whisper-small-KR\\special_tokens_map.json\n",
      "added tokens file saved in ./whisper-small-KR\\added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"Bingsu/zeroth-korean\",\n",
    "    \"dataset\": \"Zeroth-Korean\",\n",
    "    \"language\": \"ko\",\n",
    "    \"model_name\": \"Whisper Large V2 PEFT KR - BYoussef\",\n",
    "    \"finetuned_from\": \"openai/whisper-large-v2\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-KR\n",
      "Configuration saved in ./whisper-small-KR\\config.json\n",
      "Configuration saved in ./whisper-small-KR\\generation_config.json\n",
      "Model weights saved in ./whisper-small-KR\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-KR\\preprocessor_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "Upload file pytorch_model.bin:   0%|          | 32.0k/922M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38d85d5b003e46639c7a899d27292dcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload file training_args.bin: 100%|##########| 3.61k/3.61k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c128f9a9004d4095aedb1431d354e1cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload file runs/Feb22_14-36-02_DESKTOP-EVRCGBF/events.out.tfevents.1677044316.DESKTOP-EVRCGBF: 100%|#########…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ed7bbbe27c9450d9b8fdf34858e9693"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload file runs/Feb23_11-52-57_DESKTOP-EVRCGBF/1677120788.2072656/events.out.tfevents.1677120788.DESKTOP-EVRC…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "010ffdfcd7bc442eb5524cc7a16c58de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload file runs/Feb23_11-50-11_DESKTOP-EVRCGBF/events.out.tfevents.1677120618.DESKTOP-EVRCGBF: 100%|#########…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcfd52739d944b37b75da61ee0654125"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload file runs/Feb23_11-50-11_DESKTOP-EVRCGBF/1677120618.3952792/events.out.tfevents.1677120618.DESKTOP-EVRC…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "875d7fdda69e4280897dba39a6b932f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/byoussef/whisper-small-KR\n",
      "   1474173..3286e22  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}, 'dataset': {'name': 'Zeroth-Korean', 'type': 'Bingsu/zeroth-korean', 'args': 'config: kr, split: test'}}\n",
      "To https://huggingface.co/byoussef/whisper-small-KR\n",
      "   3286e22..0299e6c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'https://huggingface.co/byoussef/whisper-small-KR/commit/3286e22af19f792a746d2d14389bdc24874dbfcd'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

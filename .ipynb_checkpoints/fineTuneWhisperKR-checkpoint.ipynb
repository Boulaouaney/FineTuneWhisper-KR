{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: responses 0.18.0 has requirement urllib3>=1.25.10, but you'll have urllib3 1.25.8 which is incompatible.\u001b[0m\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-4w39jrb7\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-4w39jrb7\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: requests in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/deepnote/.local/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.27.0.dev0) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/deepnote/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.27.0.dev0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.27.0.dev0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.27.0.dev0) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/deepnote/.local/lib/python3.8/site-packages (from requests->transformers==4.27.0.dev0) (3.0.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6670979 sha256=6125adf60b284b4f6c6c7a3f2e0d15a4503f5aa54cbf85f8541534b373645a71\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cmje7u51/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
      "Successfully built transformers\n",
      "Installing collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2022.10.31 tokenizers-0.13.2 transformers-4.27.0.dev0\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
      "\u001b[K     |████████████████████████████████| 252 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 138.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.1 in /home/deepnote/.local/lib/python3.8/site-packages (from librosa) (4.5.0)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-any.whl (24 kB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Downloading lazy_loader-0.1-py3-none-any.whl (8.6 kB)\n",
      "Collecting scipy>=1.2.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 133.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /home/deepnote/.local/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 134.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.51.0\n",
      "  Downloading numba-0.56.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 133.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soxr>=0.3.2\n",
      "  Downloading soxr-0.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 116.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 45.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.14\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[K     |████████████████████████████████| 322 kB 128.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /home/deepnote/.local/lib/python3.8/site-packages (from librosa) (1.24.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/deepnote/.local/lib/python3.8/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.6 MB 127.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.9\" in /home/deepnote/.local/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (6.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.51.0->librosa) (45.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/deepnote/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/deepnote/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/deepnote/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.28.2)\n",
      "Requirement already satisfied: pycparser in /home/deepnote/.local/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.51.0->librosa) (1.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/deepnote/.local/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.1)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23692 sha256=a3cb265cf5140cacd25269a7019b1f55aea1748e787f7453ff3425c159ee75ad\n",
      "  Stored in directory: /home/deepnote/.cache/pip/wheels/0a/ed/be/49df2538fca496690a024a4374455584d65c2afd6fc3d6e9c7\n",
      "Successfully built audioread\n",
      "\u001b[31mERROR: numba 0.56.4 has requirement numpy<1.24,>=1.18, but you'll have numpy 1.24.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scipy, joblib, threadpoolctl, scikit-learn, soundfile, lazy-loader, audioread, llvmlite, numba, soxr, pooch, msgpack, librosa\n",
      "Successfully installed audioread-3.0.0 joblib-1.2.0 lazy-loader-0.1 librosa-0.10.0 llvmlite-0.39.1 msgpack-1.0.4 numba-0.56.4 pooch-1.7.0 scikit-learn-1.2.1 scipy-1.10.1 soundfile-0.12.1 soxr-0.3.4 threadpoolctl-3.1.0\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting levenshtein==0.20.2\r\n",
      "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 49.4 MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20 kB 17.3 MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 30 kB 24.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40 kB 9.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 51 kB 10.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 61 kB 11.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 71 kB 12.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 81 kB 14.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 92 kB 15.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 102 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 112 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 122 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 133 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 143 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 153 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 163 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 174 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 184 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 194 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 204 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 215 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 225 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 235 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 245 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 256 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 266 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 276 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 286 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 296 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 307 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 317 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 327 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 337 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 348 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 358 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 368 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 378 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 389 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 399 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 409 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 419 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 430 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 440 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 450 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 460 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 471 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 481 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 491 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 501 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 512 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 522 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 532 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 542 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 552 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 563 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 573 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 583 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 593 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 604 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 614 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 624 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 634 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 645 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 655 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 665 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 675 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 686 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 696 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 706 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 716 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 727 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 737 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 747 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 757 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 768 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 778 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 788 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 798 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 808 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 819 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 829 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 839 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 849 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 860 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 870 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 880 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 890 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 901 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 911 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 921 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 931 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 942 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 952 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 962 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 972 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 983 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 993 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 1.0 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 1.0 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.0 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 1.0 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 1.0 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 1.1 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.2 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.3 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.4 MB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.4 MB 12.0 MB/s \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 134.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
      "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.7\n",
      "Collecting gradio\n",
      "  Downloading gradio-3.20.0-py3-none-any.whl (14.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.3 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiofiles in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (22.1.0)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 48.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from gradio) (5.3.1)\n",
      "Requirement already satisfied: fsspec in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (2023.3.0)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[K     |████████████████████████████████| 813 kB 119.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 36.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 131.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting websockets>=10.0\n",
      "  Using cached websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from gradio) (2.10.1)\n",
      "Requirement already satisfied: aiohttp in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (3.8.4)\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-1.10.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "Requirement already satisfied: pillow in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (9.4.0)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 19.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading orjson-3.8.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 124.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (4.5.0)\n",
      "Collecting fastapi\n",
      "  Using cached fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: requests in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (2.28.2)\n",
      "Requirement already satisfied: numpy in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (1.24.2)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 21.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 128.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: markupsafe in /home/deepnote/.local/lib/python3.8/site-packages (from gradio) (2.0.1)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 41.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx->gradio) (2019.11.28)\n",
      "Requirement already satisfied: sniffio in /home/deepnote/.local/lib/python3.8/site-packages (from httpx->gradio) (1.3.0)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/deepnote/.local/lib/python3.8/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 26.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /usr/lib/python3/dist-packages (from altair>=4.2.0->gradio) (0.3)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 124.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 45.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 125.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[K     |████████████████████████████████| 965 kB 127.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/deepnote/.local/lib/python3.8/site-packages (from matplotlib->gradio) (5.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/deepnote/.local/lib/python3.8/site-packages (from matplotlib->gradio) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/deepnote/.local/lib/python3.8/site-packages (from matplotlib->gradio) (2.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->gradio) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/deepnote/.local/lib/python3.8/site-packages (from aiohttp->gradio) (1.8.2)\n",
      "Collecting h11>=0.8\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from uvicorn->gradio) (7.0)\n",
      "Collecting starlette<0.26.0,>=0.25.0\n",
      "  Using cached starlette-0.25.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->gradio) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->gradio) (1.25.8)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1; extra == \"linkify\"\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/deepnote/.local/lib/python3.8/site-packages (from pandas->gradio) (2022.7.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/deepnote/.local/lib/python3.8/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/lib/python3/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.15.5)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /home/deepnote/.local/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (1.3.10)\n",
      "Collecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->gradio) (1.14.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4709 sha256=0904ee4ceb704f41f3b185b22cdfaec1864b79dd180eba7ae811fe0e48c8bb43\n",
      "  Stored in directory: /home/deepnote/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: h11, httpcore, rfc3986, httpx, pydub, toolz, altair, mdurl, uc-micro-py, linkify-it-py, markdown-it-py, mdit-py-plugins, contourpy, pyparsing, kiwisolver, cycler, fonttools, matplotlib, websockets, uvicorn, pydantic, ffmpy, python-multipart, orjson, starlette, fastapi, pycryptodome, gradio, zipp\n",
      "Successfully installed altair-4.2.2 contourpy-1.0.7 cycler-0.11.0 fastapi-0.92.0 ffmpy-0.3.0 fonttools-4.38.0 gradio-3.20.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 kiwisolver-1.4.4 linkify-it-py-2.0.0 markdown-it-py-2.2.0 matplotlib-3.7.1 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.7 pycryptodome-3.17 pydantic-1.10.5 pydub-0.25.1 pyparsing-3.0.9 python-multipart-0.0.6 rfc3986-1.5.0 starlette-0.25.0 toolz-0.12.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets>=2.6.1\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Bingsu--zeroth-korean-787ca68963c66467\n",
      "Found cached dataset parquet (D:/Youssef/.cache/huggingface/datasets/Bingsu___parquet/Bingsu--zeroth-korean-787ca68963c66467/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Using custom data configuration Bingsu--zeroth-korean-787ca68963c66467\n",
      "Found cached dataset parquet (D:/Youssef/.cache/huggingface/datasets/Bingsu___parquet/Bingsu--zeroth-korean-787ca68963c66467/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 22263\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text'],\n",
      "        num_rows: 457\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "Zeroth_Korean = DatasetDict()\n",
    "\n",
    "Zeroth_Korean[\"train\"] = load_dataset(\"Bingsu/zeroth-korean\", split=\"train\", use_auth_token=True)\n",
    "Zeroth_Korean[\"test\"] = load_dataset(\"Bingsu/zeroth-korean\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(Zeroth_Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"Korean\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([-3.05175781e-05,  0.00000000e+00, -3.05175781e-05, ...,\n",
      "        0.00000000e+00,  0.00000000e+00, -6.10351562e-05]), 'sampling_rate': 16000}, 'text': '인사를 결정하는 과정에서 당 지도부가 우 원내대표 및 원내지도부와 충분한 상의를 거치지 않은 채 일방적으로 인사를 했다는 불만도 원내지도부를 중심으로 흘러나왔다'}\n"
     ]
    }
   ],
   "source": [
    "print(Zeroth_Korean[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_prep(batch):\n",
    "    # load audio\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # calculate log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\Youssef\\.cache\\huggingface\\datasets\\Bingsu___parquet\\Bingsu--zeroth-korean-787ca68963c66467\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-e0812d3231b56f3b.arrow\n",
      "Loading cached processed dataset at D:\\Youssef\\.cache\\huggingface\\datasets\\Bingsu___parquet\\Bingsu--zeroth-korean-787ca68963c66467\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-0f77e663e93fdffc.arrow\n"
     ]
    }
   ],
   "source": [
    "# applying data preparation func to all training examples using .map\n",
    "Zeroth_Korean = Zeroth_Korean.map(dataset_prep, remove_columns=Zeroth_Korean.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 22263\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 457\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(Zeroth_Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "         # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v2-KR\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=3000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Youssef\\Lab\\FineTuneWhisper-KR\\./whisper-small-KR is already a clone of https://huggingface.co/byoussef/whisper-small-KR. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=Zeroth_Korean[\"train\"],\n",
    "    eval_dataset=Zeroth_Korean[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in ./whisper-small-KR\\preprocessor_config.json\n",
      "tokenizer config file saved in ./whisper-small-KR\\tokenizer_config.json\n",
      "Special tokens file saved in ./whisper-small-KR\\special_tokens_map.json\n",
      "added tokens file saved in ./whisper-small-KR\\added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"Bingsu/zeroth-korean\",\n",
    "    \"dataset\": \"Zeroth-Korean\",\n",
    "    \"language\": \"ko\",\n",
    "    \"model_name\": \"Whisper Small KR - BYoussef\",\n",
    "    \"finetuned_from\": \"openai/whisper-large-v2\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-KR\n",
      "Configuration saved in ./whisper-small-KR\\config.json\n",
      "Configuration saved in ./whisper-small-KR\\generation_config.json\n",
      "Model weights saved in ./whisper-small-KR\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-KR\\preprocessor_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d85d5b003e46639c7a899d27292dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c128f9a9004d4095aedb1431d354e1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.61k/3.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7bbbe27c9450d9b8fdf34858e9693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb22_14-36-02_DESKTOP-EVRCGBF/events.out.tfevents.1677044316.DESKTOP-EVRCGBF: 100%|#########…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010ffdfcd7bc442eb5524cc7a16c58de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_11-52-57_DESKTOP-EVRCGBF/1677120788.2072656/events.out.tfevents.1677120788.DESKTOP-EVRC…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfd52739d944b37b75da61ee0654125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_11-50-11_DESKTOP-EVRCGBF/events.out.tfevents.1677120618.DESKTOP-EVRCGBF: 100%|#########…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875d7fdda69e4280897dba39a6b932f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Feb23_11-50-11_DESKTOP-EVRCGBF/1677120618.3952792/events.out.tfevents.1677120618.DESKTOP-EVRC…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/byoussef/whisper-small-KR\n",
      "   1474173..3286e22  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}, 'dataset': {'name': 'Zeroth-Korean', 'type': 'Bingsu/zeroth-korean', 'args': 'config: kr, split: test'}}\n",
      "To https://huggingface.co/byoussef/whisper-small-KR\n",
      "   3286e22..0299e6c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/byoussef/whisper-small-KR/commit/3286e22af19f792a746d2d14389bdc24874dbfcd'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
